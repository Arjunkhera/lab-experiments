{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Experiment 8*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Back Propogation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackPropagation():\n",
    "    \n",
    "    def __init__(self,networkFile=\"./inputs/structure.xlsx\",dataFile=\"./inputs/backprop.xlsx\",outputClasses=1,epochs=1000,alpha=0.1):\n",
    "        \n",
    "        # no of output classes of the network\n",
    "        self.outputClasses = outputClasses\n",
    "        # number of epochs \n",
    "        self.epochs = epochs\n",
    "        # value of learning rate\n",
    "        self.alpha = alpha\n",
    "        # to obtain random seeds\n",
    "        np.random.seed(time.time())\n",
    "\n",
    "        # Call the input function to obtain the number of nodes of each layer\n",
    "        self.networkExtract(networkFile)    \n",
    "        # Call the input function to obtain the input values from training set\n",
    "        self.dataExtract(dataFile)        \n",
    "        \n",
    "        # number of hidden layers in the network\n",
    "        self.noHiddenLayers = len(self.nodesPerLayer)\n",
    "        # add the first and last layer node counts\n",
    "        self.nodesPerLayer = [self.no_features] + self.nodesPerLayer + [self.outputClasses]\n",
    "        # total number of layers in the network\n",
    "        self.totalLayers = len(self.nodesPerLayer)\n",
    "        \n",
    "        # initialize the structure for the neural net\n",
    "        self.initialize()\n",
    "        \n",
    "        # to store the cost function for each epoch\n",
    "        self.totalerror = []\n",
    "        \n",
    "    def networkExtract(self,networkFile:str) -> None:\n",
    "        \n",
    "        # read the input from excel file\n",
    "        excel_file = networkFile\n",
    "        # convert it into a pandas dataframe\n",
    "        dataframe = pd.read_excel(excel_file)\n",
    "        # stores the number of nodes in every layer\n",
    "        self.nodesPerLayer = dataframe.columns.tolist()\n",
    "    \n",
    "\n",
    "    def dataExtract(self,dataFile:str) -> None:\n",
    "        \n",
    "        # read the input from excel file \n",
    "        excel_file = dataFile\n",
    "        # convert it into a pandas dataframe\n",
    "        dataframe = pd.read_excel(excel_file)\n",
    "        # find out the number of features\n",
    "        self.no_features = len(dataframe.columns) - 1\n",
    "        # find out the number of inputs\n",
    "        self.no_rows = len(dataframe.index)\n",
    "        \n",
    "        # Convert the dataframe into numpy array for analysis\n",
    "        self.training_data = np.array([ dataframe.iloc[i,:self.no_features].tolist() for i in range(self.no_rows) ])\n",
    "        # Obtain the output in a separate numpy column vector\n",
    "        self.actual_op = np.array([dataframe['y'].tolist()]).T\n",
    "        \n",
    "    def initialize(self):\n",
    "        \n",
    "        self.weights = []\n",
    "        # construct the weight matrices\n",
    "        for i in range(self.totalLayers-1):\n",
    "            # initialize weights randomly with mean 0 and range [-1, 1]\n",
    "            self.weights.append(2*np.random.random((self.nodesPerLayer[i]+1,self.nodesPerLayer[i+1])) - 1)            \n",
    "        \n",
    "    def run(self):\n",
    "        for i in range(self.epochs):\n",
    "            ll = self.singleCycle()\n",
    "        return ll    \n",
    "        \n",
    "    def singleCycle(self):\n",
    "        \n",
    "        # list of activations for all layers\n",
    "        activations = []\n",
    "        # list of errors for all layers\n",
    "        errors = []\n",
    "        # list of partial derivative matrices\n",
    "        delta = []\n",
    "        \n",
    "        # Forward Propogation\n",
    "        \n",
    "        # initialize with training input\n",
    "        inp = np.hstack((np.ones((self.training_data.shape[0], 1)), self.training_data))\n",
    "        activations.append(inp)\n",
    "        \n",
    "        # Propogation for rest of the layers\n",
    "        for i in range(self.totalLayers-2):\n",
    "            newinp = np.hstack((np.ones((self.training_data.shape[0], 1)),self.sigmoid(np.dot(inp,self.weights[i]))))\n",
    "            activations.append(newinp)\n",
    "            inp = newinp\n",
    "        # Do last layer separately to avoid adding bias term\n",
    "        activations.append(self.sigmoid(np.dot(inp,self.weights[self.totalLayers-2])))\n",
    "        \n",
    "        # Backward Propogation\n",
    "        err = activations[self.totalLayers-1] - self.actual_op\n",
    "        self.totalerror.append(0.5*((err.sum())**2))\n",
    "        \n",
    "        errors.append(err)\n",
    "        for i in range(self.totalLayers-2,0,-1):\n",
    "            newerr = activations[i][:,1:] * (1 - activations[i][:,1:]) * np.dot(err,self.weights[i].T[:,1:])\n",
    "            errors.append(newerr)\n",
    "            err = newerr\n",
    "        \n",
    "        # Calculate the partial derivatives\n",
    "        for i in range(self.totalLayers-1):\n",
    "            delta.append(activations[0][:,:,np.newaxis] * errors[self.totalLayers-2-i][:,np.newaxis,:])\n",
    "        \n",
    "        # Take the average of the partial derivatives\n",
    "        for i in range(self.totalLayers-1):\n",
    "            delta[i] = np.average(delta[i],axis=0)\n",
    "        \n",
    "        # Update the weights\n",
    "        for i in range(self.totalLayers-1):\n",
    "            self.weights[i] += -self.alpha * delta[i]\n",
    "            \n",
    "        return activations[self.totalLayers-1]\n",
    "    \n",
    "    def forwardPropogate(self,activations:list,inparr:list):\n",
    "                \n",
    "        curinp = inparr\n",
    "        for i in range(self.totalLayers-1):\n",
    "                curinp = self.weights\n",
    "                activations.append(np.dot(inparr))\n",
    "                \n",
    "        \n",
    "    def sigmoid(self,x:float, derivative=False) -> float:\n",
    "        \n",
    "        if (derivative == True):\n",
    "            return x * (1 - x)\n",
    "        else:\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.asarray([1,2,3])\n",
    "a.sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37851865]\n",
      " [0.7740483 ]\n",
      " [0.35766052]\n",
      " [0.76219351]\n",
      " [0.29301553]\n",
      " [0.47544113]]\n"
     ]
    }
   ],
   "source": [
    "backprop = BackPropagation(\"../inputs/structure.xlsx\",\"../inputs/backprop.xlsx\")\n",
    "print(backprop.run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.138018559672842\n"
     ]
    }
   ],
   "source": [
    "suma = 0\n",
    "for i in backprop.totalerror:\n",
    "    suma += i\n",
    "print(suma)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return x * (1 - x)\n",
    "    \n",
    "# choose a random seed for reproducible results\n",
    "np.random.seed(1)\n",
    "\n",
    "# learning rate\n",
    "alpha = .1\n",
    "\n",
    "# number of nodes in the hidden layer\n",
    "num_hidden = 3\n",
    "\n",
    "# inputs\n",
    "X = np.array([  \n",
    "    [0, 0, 1],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 1, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 1],\n",
    "])\n",
    "\n",
    "# outputs\n",
    "# x.T is the transpose of x, making this a column vector\n",
    "y = np.array([[0, 1, 0, 1, 1, 0]]).T\n",
    "\n",
    "# initialize weights randomly with mean 0 and range [-1, 1]\n",
    "# the +1 in the 1st dimension of the weight matrices is for the bias weight\n",
    "hidden_weights = 2*np.random.random((X.shape[1] + 1, num_hidden)) - 1\n",
    "output_weights = 2*np.random.random((num_hidden + 1, y.shape[1])) - 1\n",
    "\n",
    "# number of iterations of gradient descent\n",
    "num_iterations = 10000\n",
    "\n",
    "# for each iteration of gradient descent\n",
    "for i in range(num_iterations):\n",
    "\n",
    "    # forward phase\n",
    "    # np.hstack((np.ones(...), X) adds a fixed input of 1 for the bias weight\n",
    "    input_layer_outputs = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    hidden_layer_outputs = np.hstack((np.ones((X.shape[0], 1)), sigmoid(np.dot(input_layer_outputs, hidden_weights))))\n",
    "    output_layer_outputs = np.dot(hidden_layer_outputs, output_weights)\n",
    "\n",
    "    # backward phase\n",
    "    # output layer error term\n",
    "    output_error = sigmoid(output_layer_outputs) - y\n",
    "    # hidden layer error term\n",
    "    # [:, 1:] removes the bias term from the backpropagation\n",
    "    hidden_error = hidden_layer_outputs[:, 1:] * (1 - hidden_layer_outputs[:, 1:]) * np.dot(output_error, output_weights.T[:, 1:])\n",
    "\n",
    "    # partial derivatives\n",
    "    hidden_pd = input_layer_outputs[:, :, np.newaxis] * hidden_error[: , np.newaxis, :]\n",
    "    output_pd = hidden_layer_outputs[:, :, np.newaxis] * output_error[:, np.newaxis, :]\n",
    "\n",
    "    # average for total gradients\n",
    "    total_hidden_gradient = np.average(hidden_pd, axis=0)\n",
    "    total_output_gradient = np.average(output_pd, axis=0)\n",
    "\n",
    "    # update weights\n",
    "    hidden_weights += - alpha * total_hidden_gradient\n",
    "    output_weights += - alpha * total_output_gradient\n",
    "\n",
    "# print the final outputs of the neural network on the inputs X\n",
    "print(\"Output After Training: \\n{}\".format(output_layer_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackPropagation():\n",
    "    \n",
    "    def __init__(self,networkFile,dataFile,outputClasses=1,epochs=5000,alpha=0.1):\n",
    "        \n",
    "        # no of output classes of the network\n",
    "        self.outputClasses = outputClasses\n",
    "        # number of epochs \n",
    "        self.epochs = epochs\n",
    "        # value of learning rate\n",
    "        self.alpha = alpha\n",
    "        # choose a random seed for reproducible results\n",
    "        np.random.seed(1)\n",
    "\n",
    "        # Call the input function to obtain the number of nodes of each layer\n",
    "        self.networkExtract(networkFile)    \n",
    "        # Call the input function to obtain the input values\n",
    "        self.dataExtract(dataFile)        \n",
    "        \n",
    "        # number of hidden layers in the network\n",
    "        self.noHiddenLayers = len(self.nodesPerLayer)\n",
    "        # add the first and last layer node counts\n",
    "        self.nodesPerLayer = [self.no_features] + self.nodesPerLayer + [self.outputClasses]\n",
    "        # total number of layers in the network\n",
    "        self.totalLayers = len(self.nodesPerLayer)\n",
    "        \n",
    "        # initialize the structure for the neural net\n",
    "        self.initialize()\n",
    "        \n",
    "    def networkExtract(self,networkFile:str) -> None:\n",
    "        \n",
    "        # read the input from excel file\n",
    "        excel_file = networkFile\n",
    "        # convert it into a pandas dataframe\n",
    "        dataframe = pd.read_excel(excel_file)\n",
    "        # stores the number of nodes in every layer\n",
    "        self.nodesPerLayer = dataframe.columns.tolist()\n",
    "    \n",
    "\n",
    "    def dataExtract(self,dataFile:str) -> None:\n",
    "        \n",
    "        # read the input from excel file \n",
    "        excel_file = dataFile\n",
    "        # convert it into a pandas dataframe\n",
    "        dataframe = pd.read_excel(excel_file)\n",
    "        # find out the number of features\n",
    "        self.no_features = len(dataframe.columns) - 1\n",
    "        # find out the number of inputs\n",
    "        self.no_rows = len(dataframe.index)\n",
    "        \n",
    "        # Convert the dataframe into numpy array for analysis\n",
    "        self.training_data = np.array([ dataframe.iloc[i,:self.no_features].tolist() for i in range(self.no_rows) ])\n",
    "        # Obtain the output in a separate numpy column vector\n",
    "        self.actual_op = np.array([dataframe['y'].tolist()]).T\n",
    "        \n",
    "    def initialize(self):\n",
    "        \n",
    "        self.weights = []\n",
    "        # construct the weight matrices\n",
    "        for i in range(self.totalLayers-1):\n",
    "            # initialize weights randomly with mean 0 and range [-1, 1]\n",
    "            self.weights.append(2*np.random.random((self.nodesPerLayer[i]+1,self.nodesPerLayer[i+1])) - 1)            \n",
    "        \n",
    "    def run(self):\n",
    "        for i in range(self.epochs):\n",
    "            ll = self.singleCycle()\n",
    "        return ll    \n",
    "        \n",
    "    def singleCycle(self):\n",
    "        \n",
    "        # list of activations for all layers\n",
    "        activations = []\n",
    "        # list of errors for all layers\n",
    "        errors = []\n",
    "        # list of partial derivative matrices\n",
    "        delta = []\n",
    "        \n",
    "        # Forward Propogation\n",
    "        \n",
    "        # initialize with training input\n",
    "        inp = np.hstack((np.ones((self.training_data.shape[0], 1)), self.training_data))\n",
    "        activations.append(inp)\n",
    "        \n",
    "        # Propogation for rest of the layers\n",
    "        for i in range(self.totalLayers-2):\n",
    "            newinp = np.hstack((np.ones((self.training_data.shape[0], 1)),self.sigmoid(np.dot(inp,self.weights[i]))))\n",
    "            activations.append(newinp)\n",
    "            inp = newinp\n",
    "        # Do last layer separately to avoid adding bias term\n",
    "        activations.append(self.sigmoid(np.dot(inp,self.weights[self.totalLayers-2])))\n",
    "        \n",
    "        # Backward Propogation\n",
    "        err = activations[self.totalLayers-1] - self.actual_op\n",
    "        errors.append(err)\n",
    "        for i in range(self.totalLayers-2,0,-1):\n",
    "            newerr = activations[i][:,1:] * (1 - activations[i][:,1:]) * np.dot(err,self.weights[i].T[:,1:])\n",
    "            errors.append(newerr)\n",
    "            err = newerr\n",
    "        \n",
    "        # Calculate the partial derivatives\n",
    "        for i in range(self.totalLayers-1):\n",
    "            delta.append(activations[0][:,:,np.newaxis] * errors[self.totalLayers-2-i][:,np.newaxis,:])\n",
    "        \n",
    "        # Take the average of the partial derivatives\n",
    "        for i in range(self.totalLayers-1):\n",
    "            delta[i] = np.average(delta[i],axis=0)\n",
    "        \n",
    "        # Update the weights\n",
    "        for i in range(self.totalLayers-1):\n",
    "            self.weights[i] += -self.alpha * delta[i]\n",
    "            \n",
    "        return activations[self.totalLayers-1]\n",
    "    \n",
    "    def forwardPropogate(self,activations:list,inparr:list):\n",
    "                \n",
    "        curinp = inparr\n",
    "        for i in range(self.totalLayers-1):\n",
    "                curinp = self.weights\n",
    "                activations.append(np.dot(inparr))\n",
    "                \n",
    "        \n",
    "    def sigmoid(self,x:float, derivative=False) -> float:\n",
    "        \n",
    "        if (derivative == True):\n",
    "            return x * (1 - x)\n",
    "        else:\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
