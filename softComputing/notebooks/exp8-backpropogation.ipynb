{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Experiment 8*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Back Propogation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Awesome Resource](https://brilliant.org/wiki/backpropagation/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackPropagation():\n",
    "    \n",
    "    def __init__(self,networkFile,dataFile,outputClasses=1,epochs=1000,alpha=0.1):\n",
    "        \n",
    "        # no of output classes of the network\n",
    "        self.outputClasses = outputClasses\n",
    "        # number of epochs \n",
    "        self.epochs = epochs\n",
    "        # value of learning rate\n",
    "        self.alpha = alpha\n",
    "        # to obtain random seeds\n",
    "        np.random.seed(1)\n",
    "\n",
    "        # Call the input function to obtain the number of nodes of each layer\n",
    "        self.networkExtract(networkFile)    \n",
    "        # Call the input function to obtain the input values from training set\n",
    "        self.dataExtract(dataFile)        \n",
    "        \n",
    "        # number of hidden layers in the network\n",
    "        self.noHiddenLayers = len(self.nodesPerLayer)\n",
    "        # add the first and last layer node counts\n",
    "        self.nodesPerLayer = [self.no_features] + self.nodesPerLayer + [self.outputClasses]\n",
    "        # total number of layers in the network\n",
    "        self.totalLayers = len(self.nodesPerLayer)\n",
    "        \n",
    "        # initialize the structure for the neural net\n",
    "        self.initialize()\n",
    "        \n",
    "        # to store the cost function for each epoch\n",
    "        self.totalerror = []\n",
    "    \n",
    "    # Obtain the network structure\n",
    "    def networkExtract(self,networkFile:str) -> None:\n",
    "        \n",
    "        # read the input from excel file\n",
    "        excel_file = networkFile\n",
    "        # convert it into a pandas dataframe\n",
    "        dataframe = pd.read_excel(excel_file)\n",
    "        # stores the number of nodes in every layer\n",
    "        self.nodesPerLayer = dataframe.columns.tolist()\n",
    "    \n",
    "    # Obtain the training data\n",
    "    def dataExtract(self,dataFile:str) -> None:\n",
    "        \n",
    "        # read the input from excel file \n",
    "        excel_file = dataFile\n",
    "        # convert it into a pandas dataframe\n",
    "        dataframe = pd.read_excel(excel_file)\n",
    "        # find out the number of features\n",
    "        self.no_features = len(dataframe.columns) - 1\n",
    "        # find out the number of inputs\n",
    "        self.no_rows = len(dataframe.index)\n",
    "        \n",
    "        # Convert the dataframe into numpy array for analysis\n",
    "        self.training_data = np.array([ dataframe.iloc[i,:self.no_features].tolist() for i in range(self.no_rows) ])\n",
    "        # Obtain the output in a separate numpy column vector\n",
    "        self.actual_op = np.array([dataframe['y'].tolist()]).T\n",
    "    \n",
    "    # define and set the weight matrices\n",
    "    def initialize(self) -> None:\n",
    "        \n",
    "        self.weights = []\n",
    "        # construct the weight matrices\n",
    "        for i in range(self.totalLayers-1):\n",
    "            # initialize weights randomly with mean 0 and range [-1, 1] and add the bias node here itself\n",
    "            self.weights.append(2*np.random.random((self.nodesPerLayer[i]+1,self.nodesPerLayer[i+1])) - 1)            \n",
    "    \n",
    "    # train the model using backpropagation\n",
    "    def train(self) -> list:\n",
    "        for i in range(self.epochs):\n",
    "            self.singleCycle()\n",
    "    \n",
    "        return self.returnOutput()\n",
    "    \n",
    "    # test the model \n",
    "    def test(self,testinput) -> list:\n",
    "        \n",
    "        # store the testing data\n",
    "        self.testing_data = testinput\n",
    "        \n",
    "        # do forward propagation\n",
    "        self.forwardPropagation(True)\n",
    "        \n",
    "        # return the answers\n",
    "        return self.returnOutput()\n",
    "    \n",
    "    # do the following for every epoch \n",
    "    def singleCycle(self) -> None:\n",
    "        \n",
    "        # list of activations for all layers\n",
    "        self.activations = []\n",
    "        # do forward propagation\n",
    "        self.forwardPropagation()\n",
    "        \n",
    "        # list of errors for all layers\n",
    "        self.errors = []\n",
    "        # do backward propagation\n",
    "        self.backwardPropagation()\n",
    "        \n",
    "        # list of partial derivative matrices\n",
    "        self.delta = []\n",
    "        # update the weights\n",
    "        self.updateWeights()        \n",
    "        \n",
    "    \n",
    "    # Forward Propogation procedure\n",
    "    def forwardPropagation(self,trainortest=False) -> None:\n",
    "        \n",
    "        # to decide whether to test or train\n",
    "        if(trainortest == True):\n",
    "            inpData = self.testing_data\n",
    "        else:\n",
    "            inpData = self.training_data\n",
    "        \n",
    "        # initialize with training input\n",
    "        inp = np.hstack((np.ones((inpData.shape[0], 1)),inpData))\n",
    "        self.activations.append(inp)\n",
    "        \n",
    "        # Propogation for rest of the layers\n",
    "        for i in range(self.totalLayers-2):\n",
    "            newinp = np.hstack((np.ones((inpData.shape[0], 1)),self.sigmoid(np.dot(inp,self.weights[i]))))\n",
    "            self.activations.append(newinp)\n",
    "            inp = newinp\n",
    "        # Do last layer separately to avoid adding bias term\n",
    "        self.activations.append(self.sigmoid(np.dot(inp,self.weights[self.totalLayers-2])))\n",
    "        \n",
    "    # Backward Propogation procedure\n",
    "    def backwardPropagation(self) -> None:\n",
    "        \n",
    "        # error for the output layer\n",
    "        err = self.activations[self.totalLayers-1] - self.actual_op\n",
    "        \n",
    "        # errors for rest of the layers\n",
    "        self.errors.append(err)\n",
    "        for i in range(self.totalLayers-2,0,-1):\n",
    "            newerr = self.activations[i][:,1:] * (1 - self.activations[i][:,1:]) * np.dot(err,self.weights[i].T[:,1:])\n",
    "            self.errors.append(newerr)\n",
    "            err = newerr\n",
    "        \n",
    "    # Update the weights learned by gradient descent\n",
    "    def updateWeights(self) -> None:\n",
    "        # Calculate the partial derivatives\n",
    "        for i in range(self.totalLayers-1):\n",
    "            self.delta.append(self.activations[i][:,:,np.newaxis] * self.errors[self.totalLayers-2-i][:,np.newaxis,:])\n",
    "        \n",
    "        # Take the average of the partial derivatives\n",
    "        for i in range(self.totalLayers-1):\n",
    "            self.delta[i] = np.average(self.delta[i],axis=0)\n",
    "        \n",
    "        # Update the weights\n",
    "        for i in range(self.totalLayers-1):\n",
    "            self.weights[i] += -self.alpha * self.delta[i]\n",
    "    \n",
    "    # sigmoid function\n",
    "    def sigmoid(self,x,derivative=False):\n",
    "\n",
    "        if (derivative == True):\n",
    "            return x * (1 - x)\n",
    "        else:\n",
    "            return 1 / (1 + np.exp(-x))      \n",
    "    \n",
    "    def returnOutput(self) -> list:\n",
    "        \n",
    "        # for single class output\n",
    "        if(self.outputClasses == 1):\n",
    "            for i in self.activations:\n",
    "                self.op = (i >= 0.5).astype(int).tolist()\n",
    "            return self.op\n",
    "        \n",
    "        # for multiclass output\n",
    "        # initialize the output array\n",
    "        self.op =  []\n",
    "        \n",
    "        # convert the activation values into one hot vectors for multiclass outputs\n",
    "        tempArr = []\n",
    "        for i in self.activations[self.totalLayers-1]:\n",
    "            tempArr.append((i >= 0.5).astype(int))\n",
    "        \n",
    "        # convert the one hot vectors to required outputs\n",
    "        for j in tempArr:\n",
    "            self.op.append(np.argmax(j))\n",
    "        \n",
    "        return self.op\n",
    "        \n",
    "    def accuracy(self) -> float:\n",
    "        \n",
    "        match = 0\n",
    "        for i in range(len(self.actual_op)):\n",
    "            if self.op[i] == self.actual_op[i]:\n",
    "                match += 1\n",
    "        \n",
    "        return (match/len(self.actual_op))*100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing and Training on XOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [1], [1], [0]]\n",
      "accuracy : 100.0%\n",
      "[[0.49779724]\n",
      " [0.51080407]\n",
      " [0.51288275]\n",
      " [0.46590076]]\n"
     ]
    }
   ],
   "source": [
    "# provide the path to neural network structure and input file\n",
    "backprop = BackPropagation(\"../inputs/structure.xlsx\",\"../inputs/binary-xor.xlsx\")\n",
    "backprop.epochs = 3000\n",
    "backprop.alpha = 0.4\n",
    "backprop.noHiddenLayers = 2\n",
    "# to train the model\n",
    "print(backprop.train())\n",
    "# output the accuracy on training data\n",
    "print(\"accuracy : {}%\".format(backprop.accuracy()))\n",
    "# output the activation values\n",
    "print(backprop.activations[backprop.totalLayers-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "testinginput = np.array([[1,0]])\n",
    "# to train the model\n",
    "print(backprop.test(testinginput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]]\n"
     ]
    }
   ],
   "source": [
    "testinginput = np.array([[0,0]])\n",
    "# to train the model\n",
    "print(backprop.test(testinginput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]]\n"
     ]
    }
   ],
   "source": [
    "testinginput = np.array([[1,1]])\n",
    "# to train the model\n",
    "print(backprop.test(testinginput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "testinginput = np.array([[0,1]])\n",
    "# to train the model\n",
    "print(backprop.test(testinginput))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another random test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [1], [0], [1], [1], [0]]\n",
      "accuracy : 100.0%\n",
      "[[0.00311894]\n",
      " [0.99684995]\n",
      " [0.0037642 ]\n",
      " [0.9966763 ]\n",
      " [0.99098636]\n",
      " [0.00813795]]\n"
     ]
    }
   ],
   "source": [
    "# provide the path to neural network structure and input file\n",
    "backprop = BackPropagation(\"../inputs/structure.xlsx\",\"../inputs/backprop.xlsx\")\n",
    "backprop.epochs = 1000\n",
    "backprop.alpha = 0.5\n",
    "# to train the model\n",
    "print(backprop.train())\n",
    "# output the accuracy on training data\n",
    "print(\"accuracy : {}%\".format(backprop.accuracy()))\n",
    "# output the activation values\n",
    "print(backprop.activations[backprop.totalLayers-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Testing on MNIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackPropagation():\n",
    "    \n",
    "    def __init__(self,networkFile,dataFile,outputClasses=1.0,epochs=1000.0,alpha=0.1):\n",
    "        \n",
    "        # no of output classes of the network\n",
    "        self.outputClasses = outputClasses\n",
    "        # number of epochs \n",
    "        self.epochs = epochs\n",
    "        # value of learning rate\n",
    "        self.alpha = alpha\n",
    "        # to obtain random seeds\n",
    "        np.random.seed(1)\n",
    "\n",
    "        # Call the input function to obtain the number of nodes of each layer\n",
    "        self.networkExtract(networkFile)    \n",
    "        # Call the input function to obtain the input values from training set\n",
    "        self.dataExtract(dataFile)        \n",
    "        \n",
    "        # number of hidden layers in the network\n",
    "        self.noHiddenLayers = len(self.nodesPerLayer)\n",
    "        # add the first and last layer node counts\n",
    "        self.nodesPerLayer = [self.no_features] + self.nodesPerLayer + [self.outputClasses]\n",
    "        # total number of layers in the network\n",
    "        self.totalLayers = len(self.nodesPerLayer)\n",
    "        \n",
    "        print(self.nodesPerLayer)\n",
    "        \n",
    "        # initialize the structure for the neural net\n",
    "        self.initialize()\n",
    "        \n",
    "        # to store the cost function for each epoch\n",
    "        self.totalerror = []\n",
    "    \n",
    "    # Obtain the network structure\n",
    "    def networkExtract(self,networkFile:str) -> None:\n",
    "        \n",
    "        # read the input from excel file\n",
    "        excel_file = networkFile\n",
    "        # convert it into a pandas dataframe\n",
    "        dataframe = pd.read_excel(excel_file)\n",
    "        # stores the number of nodes in every layer\n",
    "        self.nodesPerLayer = dataframe.columns.tolist()\n",
    "    \n",
    "    # Obtain the training data\n",
    "    def dataExtract(self,dataFile:str) -> None:\n",
    "        \n",
    "        # read the input from excel file \n",
    "        excel_file = dataFile\n",
    "        # convert it into a pandas dataframe\n",
    "        dataframe = pd.read_csv(excel_file)\n",
    "        dataframe = dataframe[:5000]\n",
    "        # find out the number of features\n",
    "        self.no_features = len(dataframe.columns) - 1\n",
    "        # find out the number of inputs\n",
    "        self.no_rows = len(dataframe.index)\n",
    "        \n",
    "        # Convert the dataframe into numpy array for analysis\n",
    "        self.training_data = np.array([ dataframe.iloc[i,1:].tolist() for i in range(self.no_rows) ],dtype=float)\n",
    "        # Obtain the output in a separate numpy column vector\n",
    "        self.actual_op = np.array([dataframe['label'].tolist()],dtype=float).T\n",
    "        \n",
    "        print(training_data.shape)\n",
    "        print(actual_op.shape)\n",
    "    \n",
    "    # define and set the weight matrices\n",
    "    def initialize(self) -> None:\n",
    "        \n",
    "        self.weights = []\n",
    "        # construct the weight matrices\n",
    "        for i in range(self.totalLayers-1):\n",
    "            # initialize weights randomly with mean 0 and range [-1, 1] and add the bias node here itself\n",
    "            self.weights.append(2.0*np.random.random((self.nodesPerLayer[i]+1,self.nodesPerLayer[i+1])) - 1.0)            \n",
    "            self.weights[-1] = np.asfarray(self.weights[-1])\n",
    "    \n",
    "    # train the model using backpropagation\n",
    "    def train(self) -> list:\n",
    "        for i in range(self.epochs):\n",
    "            # print(\"current epoch is : {}\".format(i))\n",
    "            self.singleCycle()\n",
    "            \n",
    "        return self.returnOutput()\n",
    "    \n",
    "    # test the model \n",
    "    def test(self,testinput) -> list:\n",
    "        \n",
    "        # store the testing data\n",
    "        self.testing_data = testinput\n",
    "        \n",
    "        # do forward propagation\n",
    "        self.forwardPropagation(True)\n",
    "        \n",
    "        # return the answers\n",
    "        return self.returnOutput()\n",
    "    \n",
    "    # do the following for every epoch \n",
    "    def singleCycle(self) -> None:\n",
    "        \n",
    "        # list of activations for all layers\n",
    "        self.activations = []\n",
    "        # do forward propagation\n",
    "        self.forwardPropagation()\n",
    "        \n",
    "        # list of errors for all layers\n",
    "        self.errors = []\n",
    "        # do backward propagation\n",
    "        self.backwardPropagation()\n",
    "        \n",
    "        # list of partial derivative matrices\n",
    "        self.delta = []\n",
    "        # update the weights\n",
    "        self.updateWeights()        \n",
    "        \n",
    "    \n",
    "    # Forward Propogation procedure\n",
    "    def forwardPropagation(self,trainortest=False) -> None:\n",
    "        \n",
    "        # to decide whether to test or train\n",
    "        if(trainortest == True):\n",
    "            inpData = self.testing_data\n",
    "        else:\n",
    "            inpData = self.training_data\n",
    "        \n",
    "        # initialize with training input\n",
    "        inp = np.hstack((np.ones((inpData.shape[0], 1)),inpData))\n",
    "        self.activations.append(inp)\n",
    "        \n",
    "        # Propogation for rest of the layers\n",
    "        for i in range(self.totalLayers-2):\n",
    "            inp = np.asfarray(inp)\n",
    "            newinp = np.hstack((np.ones((inpData.shape[0], 1)),self.sigmoid(np.dot(inp,self.weights[i]))))\n",
    "            self.activations.append(newinp)\n",
    "            inp = newinp\n",
    "        # Do last layer separately to avoid adding bias term\n",
    "        self.activations.append(self.sigmoid(np.dot(inp,self.weights[self.totalLayers-2])))\n",
    "        \n",
    "    # Backward Propogation procedure\n",
    "    def backwardPropagation(self) -> None:\n",
    "        \n",
    "        # error for the output layer\n",
    "        err = self.activations[self.totalLayers-1] - self.actual_op\n",
    "        \n",
    "        # errors for rest of the layers\n",
    "        self.errors.append(err)\n",
    "        for i in range(self.totalLayers-2,0,-1):\n",
    "            err = np.asfarray(err)\n",
    "            newerr = self.activations[i][:,1:] * (1 - self.activations[i][:,1:]) * np.dot(err,self.weights[i].T[:,1:])\n",
    "            self.errors.append(newerr)\n",
    "            err = newerr\n",
    "        \n",
    "    # Update the weights learned by gradient descent\n",
    "    def updateWeights(self) -> None:\n",
    "        # Calculate the partial derivatives\n",
    "        for i in range(self.totalLayers-1):\n",
    "            self.delta.append(self.activations[i][:,:,np.newaxis] * self.errors[self.totalLayers-2-i][:,np.newaxis,:])\n",
    "            self.delta[-1] = np.asfarray(self.delta[-1])\n",
    "            \n",
    "        # Take the average of the partial derivatives\n",
    "        for i in range(self.totalLayers-1):\n",
    "            self.delta[i] = np.average(self.delta[i],axis=0)\n",
    "        \n",
    "        # Update the weights\n",
    "        for i in range(self.totalLayers-1):\n",
    "            self.weights[i] += -self.alpha * self.delta[i]\n",
    "    \n",
    "    # sigmoid function\n",
    "    def sigmoid(self,x,derivative=False):\n",
    "\n",
    "        if (derivative == True):\n",
    "            return x * (1.0 - x)\n",
    "        else:\n",
    "            return 1.0 / (1.0 + np.exp(-x))      \n",
    "    \n",
    "    def returnOutput(self) -> list:\n",
    "        \n",
    "        # for single class output\n",
    "        if(self.outputClasses == 1):\n",
    "            for i in self.activations:\n",
    "                self.op = (i >= 0.5).astype(int).tolist()\n",
    "            return self.op\n",
    "        \n",
    "        # for multiclass output\n",
    "        # initialize the output array\n",
    "        self.op =  []\n",
    "        \n",
    "        # convert the activation values into one hot vectors for multiclass outputs\n",
    "        tempArr = []\n",
    "        for i in self.activations[self.totalLayers-1]:\n",
    "            tempArr.append((i >= 0.5).astype(int))\n",
    "        \n",
    "        # convert the one hot vectors to required outputs\n",
    "        for j in tempArr:\n",
    "            self.op.append(np.argmax(j))\n",
    "        \n",
    "        return self.op\n",
    "        \n",
    "    def accuracy(self) -> float:\n",
    "        \n",
    "        match = 0\n",
    "        for i in range(len(self.actual_op)):\n",
    "            if self.op[i] == self.actual_op[i]:\n",
    "                match += 1\n",
    "        \n",
    "        return (match/len(self.actual_op))*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n",
      "(100, 1)\n",
      "[784, 100, 125, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjun/.virtualenvs/forjupyter/lib/python3.5/site-packages/ipykernel_launcher.py:171: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-5201c8b8e2f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# to train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# backprop.train()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# output the accuracy on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy : {}%\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-172-eb8190dedb52>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# print(\"current epoch is : {}\".format(i))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingleCycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturnOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-172-eb8190dedb52>\u001b[0m in \u001b[0;36msingleCycle\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-172-eb8190dedb52>\u001b[0m in \u001b[0;36mupdateWeights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# Calculate the partial derivatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotalLayers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotalLayers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masfarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# provide the path to neural network structure and input file\n",
    "backprop = BackPropagation(\"../inputs/structure.xlsx\",\"../inputs/train.csv\",10,10000,0.1)\n",
    "\n",
    "# to train the model\n",
    "# backprop.train()\n",
    "print(backprop.train())\n",
    "# output the accuracy on training data\n",
    "print(\"accuracy : {}%\".format(backprop.accuracy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
